---
title: "Communications collectives"
author: Pierre Navaro, IRMAR, CNRS.
lang: fr
format:
  html:
    code-fold: true
---

## Notions générales

Les communications collectives permettent de faire en une seule opération une
série de communications point à point.

Une communication collective concerne toujours tous les processus du
communicateur indiqué.

Pour chacun des processus, l’appel se termine lorsque la participation de celui-ci
à l’opération collective est achevée, au sens des communications point-à-point
(donc quand la zone mémoire concernée peut être modifiée).

La gestion des étiquettes dans ces communications est transparente et à la
charge du système. Elles ne sont donc jamais définies explicitement lors de
l’appel à ces sous-programmes. Cela a entre autres pour avantage que les
communications collectives n’interfèrent jamais avec les communications point à
point.

## Types de communications collectives

Il y a trois types de sous-programmes :

1. celui qui assure les synchronisations globales : `mpi.barrier(comm=0)`.
2. ceux qui ne font que transférer des données :

 - diffusion globale de données : `mpi.bcast`
 - diffusion sélective de données : `mpi.scatter`
 - collecte de données réparties : `mpi.gather`

3. ceux qui, en plus de la gestion des communications, effectuent des opérations sur les données transférées :

 - opérations de réduction (somme, produit, maximum, minimum, etc.), qu’elles soient d’un type prédéfini ou d’un type personnel : `mpi.reduce`
 - opérations de réduction avec diffusion du résultat `mpi.reduce` suivi d’un `mpi.bcast` 


## Diffusion générale `mpi.bcast`

1. Envoi, à partir de l’adresse `obj`, d’un message constitué de type `type`, par le processus `rank`, à tous les autres processus du communicateur `comm`.
2. Réception de ce message à l’adresse message pour les processus autre que `rank`.

```R
{{< include codes/bcast.R >}}
```

```bash
$ mpirun -np 4 Rscript bcast.R
vector on  3  =  1 2 3 4
vector on  2  =  1 2 3 4
vector on  1  =  1 2 3 4
vector on  0  =  1 2 3 4
```

### Exemple [pbdMPI](https://pbdr.org/documentation/pbdMPI/aa_bcast-method.html)

```R
{{< include pbdMPI/bcast.R >}}
```

```bash
$ mpirun -np 4 Rscript pbdmpi/bcast.R
COMM.RANK = 0
[1] 1 2 3 4 5
COMM.RANK = 1
[1] 0 0 0 0 0
COMM.RANK = 2
[1] 0 0 0 0 0
COMM.RANK = 3
[1] 0 0 0 0 0
COMM.RANK = 0
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    3    4    5
COMM.RANK = 1
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    3    4    5
COMM.RANK = 2
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    3    4    5
COMM.RANK = 3
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    2    3    4    5
```


## Diffusion sélective `mpi.scatter`

La ième tranche est envoyée au ième processus.

![](images/broadcastvsscatter.png)

```R
{{< include codes/scatter.R >}}
```

```bash
$ mpirun -np 4 Rscript scatter.R
data on 0 : 1 2 9 10 17 18
data on 3 : 7 8 15 16 23 24
data on 2 : 5 6 13 14 21 22
data on 1 : 3 4 11 12 19 20
```

```R
{{< include pbdMPI/scatter.R >}}
```

```bash
$ mpirun -np 4 Rscript pbdmpi/scatter.R
Original x:
COMM.RANK = 0
 [1]  1  2  3  4  5  6  7  8  9 10

Scatter list:
COMM.RANK = 0
[1] 1
COMM.RANK = 1
[1] 2 3
COMM.RANK = 2
[1] 4 5 6
COMM.RANK = 3
[1]  7  8  9 10
```

## Collecte `mpi.gather`

![](images/gather.png)

1. Envoi d'un message de chacun des processus du communicateur `comm`
2. Collecte de chacun de ces messages, par le processus `root`

Les données sont collectées dans l’ordre des rangs des processus.

## Collecte générale `mpi.allgather`

Correspond à un `mpi.gather` suivi d’un `mpi.bcast`

![](images/allgather.png)

::: {.callout-warning}
Attention avec ces fonctions `mpi.all*` elles peuvent être très gourmandes...
:::

```R
{{< include pbdMPI/gather.R >}}
```

```bash
$ mpirun -np 4 Rscript pbdmpi/gather.R
Original x:
COMM.RANK = 0
[1] 1
COMM.RANK = 1
[1] 1 2
COMM.RANK = 2
[1] 1 2 3
COMM.RANK = 3
[1] 1 2 3 4

Gather matrix:
COMM.RANK = 0
[[1]]
     [,1]
[1,]    1

[[2]]
     [,1] [,2]
[1,]    1    2

[[3]]
     [,1] [,2] [,3]
[1,]    1    2    3

[[4]]
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
```


# Réductions réparties

Une réduction est une opération appliquée à un ensemble d’éléments pour en
obtenir une seule valeur. Des exemples typiques sont la somme des éléments
d’un vecteur ou la recherche de l’élément de valeur maximum dans un vecteur

MPI propose des sous-programmes de haut-niveau pour opérer des réductions
sur des données réparties sur un ensemble de processus. Le résultat est obtenu
sur un seul processus (`mpi.reduce`) ou bien sur tous (`mpi.allreduce`,
qui est en fait équivalent à un `mpi.reduce` suivi d’un `mpi.bcast`).
Si plusieurs éléments sont concernés par processus, la fonction de réduction est
appliquée à chacun d’entre eux (par exemple à tous les éléments d’un vecteur).

## `mpi.reduce`

Opérations pour réductions réparties

 - "sum" : Somme des éléments
 - "prod" : Produit des éléments
 - "max" : Recherche du maximum
 - "min" : Recherche du minimum
 - "maxloc" : Recherche de l’indice du maximum
 - "minloc" : Recherche de l’indice du minimum

```R
{{< include codes/reduce.R >}}
```

```bash
$ mpirun -np 4 Rscript reduce.R
data on 2 : 5 6 13 14 21 22
data on 3 : 7 8 15 16 23 24
data on 0 : 1 2 9 10 17 18
data on 1 : 3 4 11 12 19 20

16 20 48 52 80 84
```

Exemple [pbdMPI](https://pbdr.org/documentation/pbdMPI/aa_reduce-method.html)

```R
{{< include pbdMPI/reduce.R >}}
```

```bash
$ mpirun -np 4 Rscript pbdmpi/reduce.R
Original x:
COMM.RANK = 0
[1] 1 2 3 4 5
COMM.RANK = 1
[1]  6  7  8  9 10
COMM.RANK = 2
[1] 11 12 13 14 15
COMM.RANK = 3
[1] 16 17 18 19 20

Reduce sum:
COMM.RANK = 0
     [,1] [,2] [,3] [,4] [,5]
[1,]   34   38   42   46   50
```
