[
  {
    "objectID": "com_collectives.html",
    "href": "com_collectives.html",
    "title": "Rmpi : Communications collectives",
    "section": "",
    "text": "Les communications collectives permettent de faire en une seule opération une série de communications point à point.\nUne communication collective concerne toujours tous les processus du communicateur indiqué.\nPour chacun des processus, l’appel se termine lorsque la participation de celui-ci à l’opération collective est achevée, au sens des communications point-à-point (donc quand la zone mémoire concernée peut être modifiée).\nLa gestion des étiquettes dans ces communications est transparente et à la charge du système. Elles ne sont donc jamais définies explicitement lors de l’appel à ces sous-programmes. Cela a entre autres pour avantage que les communications collectives n’interfèrent jamais avec les communications point à point."
  },
  {
    "objectID": "com_collectives.html#notions-générales",
    "href": "com_collectives.html#notions-générales",
    "title": "Rmpi : Communications collectives",
    "section": "",
    "text": "Les communications collectives permettent de faire en une seule opération une série de communications point à point.\nUne communication collective concerne toujours tous les processus du communicateur indiqué.\nPour chacun des processus, l’appel se termine lorsque la participation de celui-ci à l’opération collective est achevée, au sens des communications point-à-point (donc quand la zone mémoire concernée peut être modifiée).\nLa gestion des étiquettes dans ces communications est transparente et à la charge du système. Elles ne sont donc jamais définies explicitement lors de l’appel à ces sous-programmes. Cela a entre autres pour avantage que les communications collectives n’interfèrent jamais avec les communications point à point."
  },
  {
    "objectID": "com_collectives.html#types-de-communications-collectives",
    "href": "com_collectives.html#types-de-communications-collectives",
    "title": "Rmpi : Communications collectives",
    "section": "Types de communications collectives",
    "text": "Types de communications collectives\nIl y a trois types de sous-programmes :\n\ncelui qui assure les synchronisations globales : mpi.barrier(comm=0).\nceux qui ne font que transférer des données :\n\n\ndiffusion globale de données : mpi.bcast\ndiffusion sélective de données : mpi.scatter\ncollecte de données réparties : mpi.gather\n\n\nceux qui, en plus de la gestion des communications, effectuent des opérations sur les données transférées :\n\n\nopérations de réduction (somme, produit, maximum, minimum, etc.), qu’elles soient d’un type prédéfini ou d’un type personnel : mpi.reduce\nopérations de réduction avec diffusion du résultat mpi.reduce suivi d’un mpi.bcast"
  },
  {
    "objectID": "com_collectives.html#diffusion-générale-mpi.bcast",
    "href": "com_collectives.html#diffusion-générale-mpi.bcast",
    "title": "Rmpi : Communications collectives",
    "section": "Diffusion générale mpi.bcast",
    "text": "Diffusion générale mpi.bcast\n\nEnvoi, à partir de l’adresse obj, d’un message constitué de type type, par le processus rank, à tous les autres processus du communicateur comm.\nRéception de ce message à l’adresse message pour les processus autre que rank.\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm=0)\n\n# Diffusion du vecteur v sur les processeurs autre que 0\nif (id == 0) {\n    v &lt;- c(1, 2, 3, 4)\n    mpi.bcast.Robj(obj = v, rank = 0, comm = 0)\n} else {\n    v &lt;- mpi.bcast.Robj(rank = 0, comm = 0)\n}\n\ncat(\"vector on \", id, \" = \",  v, \"\\n\" )\n\ninvisible(mpi.barrier(comm=0))\ninvisible(mpi.finalize())\n\n\n\n\n\n\nAvertissement\n\n\n\nHabituellement avec MPI, le même appel mpi.bcast devrait pouvoir être effectué sur tous les processus. Je n’ai pas réussi à le faire avec Rmpi…\n\n\n$ mpirun -np 4 Rscript bcast.R\nvector on  3  =  1 2 3 4\nvector on  2  =  1 2 3 4\nvector on  1  =  1 2 3 4\nvector on  0  =  1 2 3 4"
  },
  {
    "objectID": "com_collectives.html#diffusion-sélective-mpi.scatter",
    "href": "com_collectives.html#diffusion-sélective-mpi.scatter",
    "title": "Rmpi : Communications collectives",
    "section": "Diffusion sélective mpi.scatter",
    "text": "Diffusion sélective mpi.scatter\nLa ième tranche est envoyée au ième processus.\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm=0)\nnp &lt;- mpi.comm.size(comm=0)\n\nif (id == 0) {\n    data = matrix(1:24,ncol=3)\n    splitmatrix = function(x, ncl) lapply(.splitIndices(nrow(x), ncl), function(i) x[i,])\n    chunk = splitmatrix(data, np)\n}\n\nchunk &lt;- mpi.scatter.Robj(obj = chunk, root = 0, comm = 0)\n\ncat(\"data on \", id, \":\", chunk, \"\\n\")\n\n\ninvisible(mpi.barrier(comm=0))\ninvisible(mpi.finalize())\n$ mpirun -np 4 Rscript scatter.R\ndata on 0 : 1 2 9 10 17 18\ndata on 3 : 7 8 15 16 23 24\ndata on 2 : 5 6 13 14 21 22\ndata on 1 : 3 4 11 12 19 20"
  },
  {
    "objectID": "com_collectives.html#collecte-mpi.gather",
    "href": "com_collectives.html#collecte-mpi.gather",
    "title": "Rmpi : Communications collectives",
    "section": "Collecte mpi.gather",
    "text": "Collecte mpi.gather\n\n\nEnvoi d’un message de chacun des processus du communicateur comm\nCollecte de chacun de ces messages, par le processus root\n\nLes données sont collectées dans l’ordre des rangs des processus."
  },
  {
    "objectID": "com_collectives.html#collecte-générale-mpi.allgather",
    "href": "com_collectives.html#collecte-générale-mpi.allgather",
    "title": "Rmpi : Communications collectives",
    "section": "Collecte générale mpi.allgather",
    "text": "Collecte générale mpi.allgather\nCorrespond à un mpi.gather suivi d’un mpi.bcast\n\n\n\n\n\n\n\nAvertissement\n\n\n\nAttention avec ces fonctions mpi.all* elles peuvent être très gourmandes…"
  },
  {
    "objectID": "com_collectives.html#mpi.reduce",
    "href": "com_collectives.html#mpi.reduce",
    "title": "Rmpi : Communications collectives",
    "section": "mpi.reduce",
    "text": "mpi.reduce\nOpérations pour réductions réparties\n\n“sum” : Somme des éléments\n“prod” : Produit des éléments\n“max” : Recherche du maximum\n“min” : Recherche du minimum\n“maxloc” : Recherche de l’indice du maximum\n“minloc” : Recherche de l’indice du minimum\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm=0)\nnp &lt;- mpi.comm.size(comm=0)\n\nif (id == 0) {\n    data = matrix(1:24,ncol=3)\n    splitmatrix = function(x, ncl) lapply(.splitIndices(nrow(x), ncl), function(i) x[i,])\n    chunk = splitmatrix(data, np)\n}\n\nchunk &lt;- mpi.scatter.Robj(obj = chunk, root = 0, comm = 0)\n\ncat(\"data on \", id, \":\", chunk, \"\\n\")\n\nres &lt;- mpi.reduce(chunk, type=1, op=\"sum\", dest = 0, comm = 0) \n\nif ( id == 0 ) {\n    cat(\"\\n\", res, \"\\n\")\n}\n\ninvisible(mpi.barrier(comm=0))\ninvisible(mpi.finalize())\n$ mpirun -np 4 Rscript reduce.R\ndata on 2 : 5 6 13 14 21 22\ndata on 3 : 7 8 15 16 23 24\ndata on 0 : 1 2 9 10 17 18\ndata on 1 : 3 4 11 12 19 20\n\n16 20 48 52 80 84"
  },
  {
    "objectID": "exercice.html",
    "href": "exercice.html",
    "title": "Conclusion",
    "section": "",
    "text": "Le modèle SPMD fonctionne mieux sur les clusters de calcul que le modèle MPMD\nOn supprime le surcoût des lancements de processus dans le programme principal.\nLimiter au maximum les messages avec un grand volume de données\nLimiter l’empreinte mémoire en divisant les calculs mais aussi en divisant la mémoire.\nIl est parfois plus intéressant de faire le même calcul sur tous les processus que de le faire sur un seul et ensuite faire une diffusion\nEviter de lire des données en parallèle si vous n’utilisez pas une bibliothèque dédiée (MPI-IO). Lire le fichier sur le processeur 0 puis faire un mpi.bcast ou mieux un mpi.scatter.\nEssayer d’équilibrer la charge sur vos processus\nJeter un oeil à pdbMPI. La syntaxe me semble plus sympa.\n\nDans cet exemple, on crée autant de vecteurs que de processus initialisés à la valeur du rang. Ces vecteurs sont collectés dans une matrice construite par colonnes.\nlibrary(pbdMPI, quiet = TRUE)\ninit()\n.comm.size &lt;- comm.size()\n.comm.rank &lt;- comm.rank()\n\nmsg &lt;- sprintf(\"Hello world from process %d\\n\", .comm.rank)\ncomm.cat(\"Say hello:\\n\", quiet = TRUE)\ncomm.cat(msg, all.rank = TRUE)\n\nk &lt;- 10\nx &lt;- rep(.comm.rank, k)\ncomm.cat(\"\\nOriginal x vector:\\n\", quiet = TRUE)\ncomm.print(x, all.rank = TRUE)\n\ny &lt;- allgather(x, unlist = TRUE)\nA &lt;- matrix(y, nrow = k, byrow = FALSE)\ncomm.cat(\"\\nAllgather matrix (only showing process 0):\\n\", quiet = TRUE)\ncomm.print(A)\n\nfinalize()"
  },
  {
    "objectID": "exercice.html#recommendations-pour-utiliser-mpi",
    "href": "exercice.html#recommendations-pour-utiliser-mpi",
    "title": "Conclusion",
    "section": "",
    "text": "Le modèle SPMD fonctionne mieux sur les clusters de calcul que le modèle MPMD\nOn supprime le surcoût des lancements de processus dans le programme principal.\nLimiter au maximum les messages avec un grand volume de données\nLimiter l’empreinte mémoire en divisant les calculs mais aussi en divisant la mémoire.\nIl est parfois plus intéressant de faire le même calcul sur tous les processus que de le faire sur un seul et ensuite faire une diffusion\nEviter de lire des données en parallèle si vous n’utilisez pas une bibliothèque dédiée (MPI-IO). Lire le fichier sur le processeur 0 puis faire un mpi.bcast ou mieux un mpi.scatter.\nEssayer d’équilibrer la charge sur vos processus\nJeter un oeil à pdbMPI. La syntaxe me semble plus sympa.\n\nDans cet exemple, on crée autant de vecteurs que de processus initialisés à la valeur du rang. Ces vecteurs sont collectés dans une matrice construite par colonnes.\nlibrary(pbdMPI, quiet = TRUE)\ninit()\n.comm.size &lt;- comm.size()\n.comm.rank &lt;- comm.rank()\n\nmsg &lt;- sprintf(\"Hello world from process %d\\n\", .comm.rank)\ncomm.cat(\"Say hello:\\n\", quiet = TRUE)\ncomm.cat(msg, all.rank = TRUE)\n\nk &lt;- 10\nx &lt;- rep(.comm.rank, k)\ncomm.cat(\"\\nOriginal x vector:\\n\", quiet = TRUE)\ncomm.print(x, all.rank = TRUE)\n\ny &lt;- allgather(x, unlist = TRUE)\nA &lt;- matrix(y, nrow = k, byrow = FALSE)\ncomm.cat(\"\\nAllgather matrix (only showing process 0):\\n\", quiet = TRUE)\ncomm.print(A)\n\nfinalize()"
  },
  {
    "objectID": "exercice.html#exercice",
    "href": "exercice.html#exercice",
    "title": "Conclusion",
    "section": "Exercice",
    "text": "Exercice\nOn veut paralléliser l’exemple suivant avec Rmpi que j’ai trouvé ici\nmy.loop &lt;- 20\nm.dim &lt;- list(nrow = 200000, ncol = 10)\nm &lt;- matrix(1, nrow = m.dim$nrow, ncol = m.dim$ncol)\nret &lt;- 0\n\nstart &lt;- Sys.time()\nfor(k in 1 : my.loop){\n  ret &lt;- ret + sum(rowSums(m))\n}\nSys.time() - start\nVoici la version MPMD venant de la même source, essayer d’en faire une version SPMD et comparer les performances.\n\nPouvez-vous réduire le nombre de messages ?\nVotre code est-il plus conçis ?\n\n\nloop.fun &lt;- function(){\n  m.dim &lt;- list(nrow = 200000, ncol = 10)\n  m &lt;- matrix(1, nrow = m.dim$nrow, ncol = m.dim$ncol)\n  ret &lt;- sum(rowSums(m))\n}\n\ncall.mpi.master &lt;- function(){\n  library(Rmpi)\n  mpi.spawn.Rslaves(needlog = FALSE)\n  mpi.bcast.Robj2slave(call.mpi.slave)\n  mpi.bcast.cmd(call.mpi.slave())\n  mpi.bcast.Robj(loop.fun)\n\n  my.size &lt;- mpi.universe.size()\n  my.loop &lt;- 20 \n  my.split &lt;- data.frame(loop = 1 : my.loop,\n                         rank = sort(c(rep(1 : my.size, my.loop %/% my.size),\n                                if(my.loop %% my.size &gt; 0)\n                                (my.size : 2)[1 : (my.loop %% my.size)])))\n\n  for(i in 1 : my.size){\n    mpi.send.Robj(my.split$loop[my.split$rank == i], dest = i, tag = 1)\n  }\n\n  ret &lt;- 0\n  for(i in 1 : my.size){\n    ret &lt;- ret + mpi.recv(integer(1), type = 1, source = i, tag = 2)\n  }\n  ret\n}\n\ncall.mpi.slave &lt;- function(){\n  loop.fun &lt;- eval(mpi.bcast.Robj())\n  my.loop &lt;- mpi.recv.Robj(source = 0, tag = 1)\n\n  ret &lt;- 0\n  for(i in my.loop){\n    ret &lt;- ret + loop.fun()\n  }\n  mpi.send(as.integer(ret), type = 1, dest = 0, tag = 2)\n}\n\nstart &lt;- Sys.time()\ncall.mpi.master()\nSys.time() - start"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rmpi",
    "section": "",
    "text": "Ces supports reprennent très largement les supports de cours MPI de l’IDRIS écrits par Dimitri Lecas, Rémi Lacroix, Serge Van Criekingen et Myriam Peyrounette. J’ai utilisé également les ressources suivantes:"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Rmpi",
    "section": "Introduction",
    "text": "Introduction\nL’utilisation de la bibliothèque MPI permet d’exploiter le parallélisme des ordinateurs en utilisant le paradigme de l’échange de messages.\nOn parle de programme séquentiel lorsqu’il est exécuté par un et un seul processus. Dans ce cas, toutes les variables et constantes sont allouées dans la mémoire allouée au processus\nDans un programme parallèle par échanges de messages\n\nLe programme est exécuté simultanément dans plusieurs processus.\nToutes les variables sont privées et résident dans la mémoire locale allouée à chaque processus.\nChaque processus exécute éventuellement des parties différentes d’un programme.\nUne donnée est échangée entre deux ou plusieurs processus via des appels de fonctions."
  },
  {
    "objectID": "index.html#léchange-de-messages",
    "href": "index.html#léchange-de-messages",
    "title": "Rmpi",
    "section": "L’échange de messages",
    "text": "L’échange de messages\nLe message est constitué de paquets de données transitant du processus émetteur au(x) processus récepteur(s). Il devra contenir:\n\nLes données (variables scalaires, tableaux, etc.)\nl’identificateur du processus émetteur\nle type de la donnée\nl’identificateur du processus récepteur."
  },
  {
    "objectID": "index.html#architecture-des-supercalculateurs",
    "href": "index.html#architecture-des-supercalculateurs",
    "title": "Rmpi",
    "section": "Architecture des supercalculateurs",
    "text": "Architecture des supercalculateurs\nLa plupart des supercalculateurs sont des machines à mémoire distribuée. Ils sont composés d’un ensemble de nœud, à l’intérieur d’un nœud la mémoire est partagée."
  },
  {
    "objectID": "index.html#mpi-vs-openmp",
    "href": "index.html#mpi-vs-openmp",
    "title": "Rmpi",
    "section": "MPI vs OpenMP",
    "text": "MPI vs OpenMP\nOpenMP utilise un schéma à mémoire partagée, tandis que pour MPI la mémoire est distribuée."
  },
  {
    "objectID": "index.html#historique",
    "href": "index.html#historique",
    "title": "Rmpi",
    "section": "Historique",
    "text": "Historique\n\nVersion 1.0 : en juin 1994, le forum MPI, avec la participation d’une quarantaine d’organisations, aboutit à la définition d’un ensemble de sous-programmes concernant la bibliothèque d’échanges de messagesMPI\nVersion 2.0 : apparue en juillet 1997, cette version apportait des compléments importants volontairement non intégrés dans MPI 1.0 (gestion dynamique de processus, copies mémoire à mémoire, entrées-sorties parallèles, etc.)\nVersion 3.0 : septembre 2012, cette version apportait les communications collectives non bloquantes, nouvelle interface Fortran, etc.\nVersion 4.0 : juin 2021"
  },
  {
    "objectID": "index.html#implémentations-mpi",
    "href": "index.html#implémentations-mpi",
    "title": "Rmpi",
    "section": "Implémentations MPI",
    "text": "Implémentations MPI\n\nMPICH\nOpen MPI\n\nBibliothèques scientifiques parallèles\nHDF5 : Lecture et écriture sur fichiers."
  },
  {
    "objectID": "index.html#anatomie-dun-programme-mpi",
    "href": "index.html#anatomie-dun-programme-mpi",
    "title": "Rmpi",
    "section": "Anatomie d’un programme MPI",
    "text": "Anatomie d’un programme MPI\n\ninitialiser l’environnement : initialize\ncommunicateur : comm\nrang : rank\nnombre de processus : size\nfermer : finalize"
  },
  {
    "objectID": "index.html#exemple-c",
    "href": "index.html#exemple-c",
    "title": "Rmpi",
    "section": "Exemple C",
    "text": "Exemple C\nCloner le dépôt https://github.com/GroupeCalcul/ANFRCalculRmpi pour récupérer les exemples.\n#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    // Initialize the MPI environment\n    MPI_Init(NULL, NULL);\n\n    // Get the number of processes\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    // Get the rank of the process\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Get the name of the processor\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &name_len);\n\n    // Print off a hello world message\n    printf(\"Hello world from processor %s, rank %d out of %d processors\\n\",\n           processor_name, world_rank, world_size);\n\n    // Finalize the MPI environment.\n    MPI_Finalize();\n}"
  },
  {
    "objectID": "index.html#compilation-et-exécution-dun-code-mpi-en-c",
    "href": "index.html#compilation-et-exécution-dun-code-mpi-en-c",
    "title": "Rmpi",
    "section": "Compilation et exécution d’un code MPI en C",
    "text": "Compilation et exécution d’un code MPI en C\nPour compiler un code MPI, il faut faire le lien avec la librairie MPI utilisée en utilisant par exemple mpicc\n&gt; mpicc hello_mpi.c -o hello\n\n\n\n\n\n\nNote\n\n\n\nPackages ubuntu : libopenmpi-devet openmpi-bin\n\n\nPour exécuter un code MPI, on utilise un lanceur d’application MPI qui ordonne le lancement de l’exécution sur un nombre de processus choisi. Le lanceur défini par la norme MPI est mpiexec. Il existe également des lanceurs non standards, comme mpirun.\n&gt; mpiexec -n 4 ./hello\nHello world from processor ar039133.math.univ-rennes1.fr, rank 2 out of 4 processors\nHello world from processor ar039133.math.univ-rennes1.fr, rank 3 out of 4 processors\nHello world from processor ar039133.math.univ-rennes1.fr, rank 1 out of 4 processors\nHello world from processor ar039133.math.univ-rennes1.fr, rank 0 out of 4 processors"
  },
  {
    "objectID": "index.html#exemple-python",
    "href": "index.html#exemple-python",
    "title": "Rmpi",
    "section": "Exemple Python",
    "text": "Exemple Python\nPour les langages interprétés, il est nécessaire de lancer plusieurs sessions avec mpiexec ou mpirun pour utiliser MPI.\n#!/usr/bin/env python\n\nfrom mpi4py import MPI\n\nsize = MPI.COMM_WORLD.Get_size()\nrank = MPI.COMM_WORLD.Get_rank()\nname = MPI.Get_processor_name()\n\nprintln(f\"Hello, World! I am process {rank} of {size} on {name}\")\n&gt; mpiexec -n 4 python hello_mpi.py                     \nHello, World! I am process 2 of 4 on ar039133.math.univ-rennes1.fr\nHello, World! I am process 1 of 4 on ar039133.math.univ-rennes1.fr\nHello, World! I am process 3 of 4 on ar039133.math.univ-rennes1.fr\nHello, World! I am process 0 of 4 on ar039133.math.univ-rennes1.fr"
  },
  {
    "objectID": "index.html#exemple-r-spmd-single-program-multiple-data",
    "href": "index.html#exemple-r-spmd-single-program-multiple-data",
    "title": "Rmpi",
    "section": "Exemple R SPMD (Single Program Multiple Data)",
    "text": "Exemple R SPMD (Single Program Multiple Data)\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm=0)\nnp &lt;- mpi.comm.size(comm=0)\nhostname &lt;- mpi.get.processor.name()\n\nmsg &lt;- sprintf (\"Hello world from task %03d of %03d, on host %s \\n\", id , np , hostname)\ncat(msg)\n\ninvisible(mpi.barrier(comm=0))\ninvisible(mpi.finalize())\n\n\n\n\n\n\nNote\n\n\n\nPour une raison qui m’est inconnue, si on laisse la valeur comm=1 par défaut, l’exemple ne fonctionne pas.\n\n\nInstallation de l’environnement logiciel\n$ Rscript -e 'install.packages(\"Rmpi\")'\n$ mpiexec -np 4 Rscript hello_mpi.R\nHello world from task 003 of 004, on host srv-mingus\nHello world from task 000 of 004, on host srv-mingus\nHello world from task 001 of 004, on host srv-mingus\nHello world from task 002 of 004, on host srv-mingus"
  },
  {
    "objectID": "index.html#exécution-sur-le-cluster-perseus",
    "href": "index.html#exécution-sur-le-cluster-perseus",
    "title": "Rmpi",
    "section": "Exécution sur le cluster perseus",
    "text": "Exécution sur le cluster perseus\nInstallation de Rmpi et récupération du matériel\nsource /applis/environments/conda.sh\nconda create -y -n rmpi r-rmpi -c conda-forge\nconda activate rmpi\ngit clone https://github.com/GroupeCalcul/ANFRCalculRmpi\ncd ANFRCalculRmpi\nfichier hello_mpi.sh nécessaire à l’utilisation d’OAR\n##!/bin/bash\n#OAR --project pr-groupecalcul\n#OAR -n hello_mpi\n#OAR -l /nodes=1/core=4,walltime=00:01:00\n#OAR --stdout hello_mpi.out\n#OAR --stderr hello_mpi.err\n\n## Ensure conda is loaded. The following line can be into your ~/.bashrc file.\nsource /applis/environments/conda.sh\n\n## Run the program\nconda activate rmpi\nmpirun --np 4 --machinefile $OAR_NODE_FILE --mca plm_rsh_agent \"oarsh\" $OAR_WORKING_DIRECTORY/hello_mpi\nmpirun --np 4 --machinefile $OAR_NODE_FILE --mca plm_rsh_agent \"oarsh\" Rscript $OAR_WORKING_DIRECTORY/hello_mpi.R\nCe script doit être exécutable\n$ chmod +x hello_mpi.sh\n$ ls -l hello_mpi.sh\n-rwxr-xr-x 1 login-perseus l-formations 567 Sep 24 14:30 hello_mpi.sh\n$ oarsub -S ./hello_mpi.sh\n[ADMISSION RULE] Antifragmentation activated\n[ADMISSION RULE] No antifrag for small jobs\n[FAST] Adding fast resource constraints\n[PARALLEL] Small jobs (&lt; 32 cores) restricted to tagged nodes\n[ADMISSION RULE] Modify resource description with type constraints\n[ADMISSION RULE] Found job type [verysmall]\n[ADMISSION RULE] Automatically add job type [verysmall]\nOAR_JOB_ID=26118495\n$ oarstat -u\nJob id    S User     Duration   System message\n--------- - -------- ---------- ------------------------------------------------\n26118496  W navarop-    0:00:00 R=4,W=0:1:0,J=B,N=hello_rmpi,P=groupecalcul,T=heterogeneous|verysmall\n$ oarstat -u\nJob id    S User     Duration   System message\n--------- - -------- ---------- ------------------------------------------------\n26118496  L navarop-    0:00:03 R=4,W=0:1:0,J=B,N=hello_rmpi,P=groupecalcul,T=heterogeneous|verysmall (Karma=0.004,quota_ok)\n$ oarstat -u\nJob id    S User     Duration   System message\n--------- - -------- ---------- ------------------------------------------------\n26118496  R navarop-    0:00:06 R=4,W=0:1:0,J=B,N=hello_rmpi,P=groupecalcul,T=heterogeneous|verysmall (Karma=0.004,quota_ok)\n$ oarstat -u\nJob id    S User     Duration   System message\n--------- - -------- ---------- ------------------------------------------------\n26118496  F navarop-    0:00:18 R=4,W=0:1:0,J=B,N=hello_rmpi,P=groupecalcul,T=heterogeneous|verysmall (Karma=0.004,quota_ok)\n$ cat hello_mpi.out\nHello world from processor dahu146, rank 3 out of 4 processors\nHello world from processor dahu146, rank 2 out of 4 processors\nHello world from processor dahu146, rank 0 out of 4 processors\nHello world from processor dahu146, rank 1 out of 4 processors\nHello world from task 003 of 004, on host dahu146\nHello world from task 000 of 004, on host dahu146\nHello world from task 002 of 004, on host dahu146\nHello world from task 001 of 004, on host dahu146"
  },
  {
    "objectID": "index.html#exemple-r-mpmd-multiple-program-multiple-data",
    "href": "index.html#exemple-r-mpmd-multiple-program-multiple-data",
    "title": "Rmpi",
    "section": "Exemple R MPMD (Multiple Program Multiple Data)",
    "text": "Exemple R MPMD (Multiple Program Multiple Data)\n\nlibrary(Rmpi)\n\nmpi.spawn.Rslaves(nslaves = 4, needlog = FALSE)\n\nmpi.bcast.cmd( id &lt;- mpi.comm.rank() )\nmpi.bcast.cmd( np &lt;- mpi.comm.size() )\nmpi.bcast.cmd( host &lt;- mpi.get.processor.name() )\nresult &lt;- mpi.remote.exec(paste(\"I am\", id, \"of\", np, \"running on\", host)) \n\nprint(unlist(result))\n\nmpi.close.Rslaves(dellog = FALSE)\nmpi.exit()\n$ Rscript hello_mpmd.R\n4 slaves are spawned successfully. 0 failed.\nmaster (rank 0, comm 1) of size 5 is running on: sr036124\nslave1 (rank 1, comm 1) of size 5 is running on: sr036124\nslave2 (rank 2, comm 1) of size 5 is running on: sr036124\nslave3 (rank 3, comm 1) of size 5 is running on: sr036124\nslave4 (rank 4, comm 1) of size 5 is running on: sr036124\n\n\n\n\n\n\nAvertissement\n\n\n\nJe n’ai pas réussi à faire tourner cet exemple dans l’environnement conda. En revanche cela fonctionne sur Linux avec le package ubuntu r-cran-rmpi."
  },
  {
    "objectID": "index.html#spmd-vs-mpmd",
    "href": "index.html#spmd-vs-mpmd",
    "title": "Rmpi",
    "section": "SPMD vs MPMD",
    "text": "SPMD vs MPMD\nOn retrouve plus d’exemples utilisant la méthode MPMD mais cette forme de parallélisation est plus difficile à faire fonctionner et semble moins rapide (voir ici). La technique SPMD présente plusieurs avantages:\n\nCode plus proche du code séquentiel, c’est-à-dire que le SPMD est plus facile à coder à partir de la version séquentielle\nCode plus court que la version MPMD, donc moins d’erreurs potentielles.\nLe processeur 0 travaille également, ce qui permet d’utiliser pleinement les ressources.\nEn général la taille et nombre de messages sont plus réduits."
  },
  {
    "objectID": "index.html#exercice",
    "href": "index.html#exercice",
    "title": "Rmpi",
    "section": "Exercice",
    "text": "Exercice\nImplémenter un programme MPI SPMD dans lequel chaque processus affiche un message indiquant si son rang est pair ou impair. Par exemple :\n&gt; mpiexec -np 4 Rscript pair_impair.R\nMoi, processus 0, je suis de rang pair\nMoi, processus 1, je suis de rang impair\nMoi, processus 2, je suis de rang pair\nMoi, processus 3, je suis de rang impair"
  },
  {
    "objectID": "com_point_a_point.html",
    "href": "com_point_a_point.html",
    "title": "Rmpi : communications point à point",
    "section": "",
    "text": "Une communication dite point à point a lieu entre deux processus, l’un appelé processus émetteur et l’autre processus récepteur (ou destinataire). L’émetteur et le récepteur sont identifiés par leur rang dans le communicateur. L’entité transmise entre deux processus est appelée message. Un message est caractérisé par son enveloppe. Celle-ci est constituée :\n\ndu rang du processus émetteur source;\ndu rang du processus récepteur dest ;\nde l’étiquette du message tag;\ndu communicateur qui définit le groupe de processus et le contexte de communication comm.\nLes données échangées sont typées (entiers, réels, etc…) type.\n\nCette opération est bloquante : l’exécution reste bloquée jusqu’à ce que le contenu de message puisse être réécrit sans risque d’écraser la valeur qui devait être envoyée.\n\nmpi.send(x, type, dest, tag,  comm = 1)\nmpi.recv(x, type, source, tag,  comm = 1, status = 0)\n\ntype : 1 pour les entiers, 2 pour les flottants, 3 pour les caractères.\nL’appel mpi.recv pourra fonctionner avec une opération mpi.send si ces deux appels ont la même enveloppe (source, dest, tag, comm).\nCette opération est bloquante : l’exécution reste bloquée jusqu’à ce que le contenu de message corresponde au message reçu.\n\n\n\n\n\n\nNote\n\n\n\nLes fonctions mpi.send.Robj et mpi.recv.Robj permettent d’envoyer et recevoir des objets R à condition qu’ils soient sérialisables comme les data.frame par exemple. Il existe des versions non bloquantes mpi.isend et mpi.irecv ainsi qu’une version contractée mpi.sendrecv bidirectionnel. Ces concepts ne seront pas abordés ici.\n\n\n\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm = 0)\nnp &lt;- mpi.comm.size(comm = 0)\n\nif (id == 0) {\n    for (i in seq(1, np-1)) {\n        recv &lt;- mpi.recv(x = integer(1), type = 1, source = i, tag = 0, comm = 0)\n        cat(\"Process 0: Received from process\", i, \"saying:\", recv, \"\\n\")\n    }\n} else {\n    msg &lt;- as.integer(id)\n    cat(\"Process\", id, \" sent to process 0:\", msg, \"\\n\")\n    invisible(mpi.send(msg, 1, dest = 0, tag = 0, comm = 0))\n}\n\ninvisible(mpi.barrier(comm = 0))\ninvisible(mpi.finalize())\n$ mpiexec -np 4 Rscript send_recv.R\nProcess 3  sent to process 0: 3\nProcess 2  sent to process 0: 2\nProcess 1  sent to process 0: 1\nProcess 0: Received from process 1 saying: 1\nProcess 0: Received from process 2 saying: 2\nProcess 0: Received from process 3 saying: 3\n\n\n\nL’exercice est décomposé en 3 étapes :\n\nPing: compléter le script ping_pong_1.R de manière à ce que le processus de rang 0 envoie un message contenant une série aléatoire de 1000 réels au rang 1.\nPing-Pong: compléter le script ping_pong_2.R de manière à ce que le processus de rang 1 renvoie le message vers le processus de rang 0.\nMatch de Ping-Pong: compléter le script ping_pong_3.R de manière à enchainer 9 échanges."
  },
  {
    "objectID": "com_point_a_point.html#exemple",
    "href": "com_point_a_point.html#exemple",
    "title": "Rmpi : communications point à point",
    "section": "",
    "text": "library(Rmpi)\n\nid &lt;- mpi.comm.rank(comm = 0)\nnp &lt;- mpi.comm.size(comm = 0)\n\nif (id == 0) {\n    for (i in seq(1, np-1)) {\n        recv &lt;- mpi.recv(x = integer(1), type = 1, source = i, tag = 0, comm = 0)\n        cat(\"Process 0: Received from process\", i, \"saying:\", recv, \"\\n\")\n    }\n} else {\n    msg &lt;- as.integer(id)\n    cat(\"Process\", id, \" sent to process 0:\", msg, \"\\n\")\n    invisible(mpi.send(msg, 1, dest = 0, tag = 0, comm = 0))\n}\n\ninvisible(mpi.barrier(comm = 0))\ninvisible(mpi.finalize())\n$ mpiexec -np 4 Rscript send_recv.R\nProcess 3  sent to process 0: 3\nProcess 2  sent to process 0: 2\nProcess 1  sent to process 0: 1\nProcess 0: Received from process 1 saying: 1\nProcess 0: Received from process 2 saying: 2\nProcess 0: Received from process 3 saying: 3"
  },
  {
    "objectID": "com_point_a_point.html#exercice-ping-pong",
    "href": "com_point_a_point.html#exercice-ping-pong",
    "title": "Rmpi : communications point à point",
    "section": "",
    "text": "L’exercice est décomposé en 3 étapes :\n\nPing: compléter le script ping_pong_1.R de manière à ce que le processus de rang 0 envoie un message contenant une série aléatoire de 1000 réels au rang 1.\nPing-Pong: compléter le script ping_pong_2.R de manière à ce que le processus de rang 1 renvoie le message vers le processus de rang 0.\nMatch de Ping-Pong: compléter le script ping_pong_3.R de manière à enchainer 9 échanges."
  }
]