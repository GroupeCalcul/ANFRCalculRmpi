[
  {
    "objectID": "com_collectives.html",
    "href": "com_collectives.html",
    "title": "Communications collectives",
    "section": "",
    "text": "Les communications collectives permettent de faire en une seule opération une série de communications point à point.\nUne communication collective concerne toujours tous les processus du communicateur indiqué.\nPour chacun des processus, l’appel se termine lorsque la participation de celui-ci à l’opération collective est achevée, au sens des communications point-à-point (donc quand la zone mémoire concernée peut être modifiée).\nLa gestion des étiquettes dans ces communications est transparente et à la charge du système. Elles ne sont donc jamais définies explicitement lors de l’appel à ces sous-programmes. Cela a entre autres pour avantage que les communications collectives n’interfèrent jamais avec les communications point à point."
  },
  {
    "objectID": "com_collectives.html#notions-générales",
    "href": "com_collectives.html#notions-générales",
    "title": "Communications collectives",
    "section": "",
    "text": "Les communications collectives permettent de faire en une seule opération une série de communications point à point.\nUne communication collective concerne toujours tous les processus du communicateur indiqué.\nPour chacun des processus, l’appel se termine lorsque la participation de celui-ci à l’opération collective est achevée, au sens des communications point-à-point (donc quand la zone mémoire concernée peut être modifiée).\nLa gestion des étiquettes dans ces communications est transparente et à la charge du système. Elles ne sont donc jamais définies explicitement lors de l’appel à ces sous-programmes. Cela a entre autres pour avantage que les communications collectives n’interfèrent jamais avec les communications point à point."
  },
  {
    "objectID": "com_collectives.html#types-de-communications-collectives",
    "href": "com_collectives.html#types-de-communications-collectives",
    "title": "Communications collectives",
    "section": "Types de communications collectives",
    "text": "Types de communications collectives\nIl y a trois types de sous-programmes :\n\ncelui qui assure les synchronisations globales : mpi.barrier(comm=0).\nceux qui ne font que transférer des données :\n\n\ndiffusion globale de données : mpi.bcast\ndiffusion sélective de données : mpi.scatter\ncollecte de données réparties : mpi.gather\ncollecte par tous les processus de données réparties : mpi.allgather\n\n\nceux qui, en plus de la gestion des communications, effectuent des opérations sur les données transférées :\n\n\nopérations de réduction (somme, produit, maximum, minimum, etc.), qu’elles soient d’un type prédéfini ou d’un type personnel : mpi.reduce\nopérations de réduction avec diffusion du résultat mpi.reduce suivi d’un mpi.bcast"
  },
  {
    "objectID": "com_collectives.html#diffusion-générale-mpi.bcast",
    "href": "com_collectives.html#diffusion-générale-mpi.bcast",
    "title": "Communications collectives",
    "section": "Diffusion générale mpi.bcast",
    "text": "Diffusion générale mpi.bcast\n\nEnvoi, à partir de l’adresse message, d’un message constitué de longueur élément de type type_message, par le processus rank, à tous les autres processus du communicateur comm.\nRéception de ce message à l’adresse message pour les processus autre que rank.\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm=0)\n\n# Diffusion du vecteur v sur les processeurs autre que 0\nif (id == 0) {\n    v &lt;- c(1, 2, 3, 4)\n    mpi.bcast.Robj(obj = v, rank = 0, comm = 0)\n} else {\n    v &lt;- mpi.bcast.Robj(rank = 0, comm = 0)\n}\n\ncat(\"vector on \", id, \" = \",  v, \"\\n\" )\n\ninvisible(mpi.barrier(comm=0))\ninvisible(mpi.finalize())\n$ mpirun -np 4 Rscript bcast.R\nvector on  3  =  1 2 3 4\nvector on  2  =  1 2 3 4\nvector on  1  =  1 2 3 4\nvector on  0  =  1 2 3 4"
  },
  {
    "objectID": "com_collectives.html#diffusion-sélective-mpi.scatter",
    "href": "com_collectives.html#diffusion-sélective-mpi.scatter",
    "title": "Communications collectives",
    "section": "Diffusion sélective mpi.scatter",
    "text": "Diffusion sélective mpi.scatter\nLa ième tranche est envoyée au ième processus.\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm=0)\nnp &lt;- mpi.comm.size(comm=0)\n\nif (id == 0) {\n    data = matrix(1:24,ncol=3)\n    splitmatrix = function(x, ncl) lapply(.splitIndices(nrow(x), ncl), function(i) x[i,])\n    chunk = splitmatrix(data, np)\n}\n\nchunk &lt;- mpi.scatter.Robj(obj = chunk, root = 0, comm = 0)\n\ncat(\"data on \", id, \":\", chunk, \"\\n\")\n\n\ninvisible(mpi.barrier(comm=0))\ninvisible(mpi.finalize())\n$ mpirun -np 4 Rscript scatter.R\ndata on  0 : 1 2 9 10 17 18\ndata on  3 : 7 8 15 16 23 24\ndata on  2 : 5 6 13 14 21 22\ndata on  1 : 3 4 11 12 19 20"
  },
  {
    "objectID": "com_collectives.html#collecte-mpi.gather",
    "href": "com_collectives.html#collecte-mpi.gather",
    "title": "Communications collectives",
    "section": "Collecte mpi.gather",
    "text": "Collecte mpi.gather\n\n\nEnvoi d’un message de chacun des processus du communicateur comm\nCollecte de chacun de ces messages, par le processus root\n\nLes données sont collectées dans l’ordre des rangs des processus."
  },
  {
    "objectID": "com_collectives.html#collecte-générale-mpi.allgather",
    "href": "com_collectives.html#collecte-générale-mpi.allgather",
    "title": "Communications collectives",
    "section": "Collecte générale mpi.allgather",
    "text": "Collecte générale mpi.allgather\nCorrespond à un mpi.gather suivi d’un mpi.bcast"
  },
  {
    "objectID": "com_collectives.html#mpi.reduce",
    "href": "com_collectives.html#mpi.reduce",
    "title": "Communications collectives",
    "section": "mpi.reduce",
    "text": "mpi.reduce\nOpérations pour réductions réparties\n\n“sum” Somme des éléments\n“prod” Produit des éléments\n“max” Recherche du maximum\n“min” Recherche du minimum\n“maxloc” Recherche de l’indice du maximum\n“minloc” Recherche de l’indice du minimum\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm=0)\nnp &lt;- mpi.comm.size(comm=0)\n\nif (id == 0) {\n    data = matrix(1:24,ncol=3)\n    splitmatrix = function(x, ncl) lapply(.splitIndices(nrow(x), ncl), function(i) x[i,])\n    chunk = splitmatrix(data, np)\n}\n\nchunk &lt;- mpi.scatter.Robj(obj = chunk, root = 0, comm = 0)\n\ncat(\"data on \", id, \":\", chunk, \"\\n\")\n\nres &lt;- mpi.reduce(chunk, type=1, op=\"sum\", dest = 0, comm = 0) \n\nif ( id == 0 ) {\n    cat(\"\\n\", res, \"\\n\")\n}\n\ninvisible(mpi.barrier(comm=0))\ninvisible(mpi.finalize())\n$ mpirun -np 4 Rscript reduce.R\ndata on  2 : 5 6 13 14 21 22\ndata on  3 : 7 8 15 16 23 24\ndata on  0 : 1 2 9 10 17 18\ndata on  1 : 3 4 11 12 19 20\n\n16 20 48 52 80 84"
  },
  {
    "objectID": "exercice.html",
    "href": "exercice.html",
    "title": "Exercice",
    "section": "",
    "text": "Le modèle SPMD fonctionne mieux sur les clusters de calcul que le modèle SPMD\nLimiter au maximum les messages avec un grand volumes de données\nLimiter l’empreinte mémoire en divisant les calculs mais aussi en divisant la mémoire.\nIl est parfois plus intéressant de faire le même calcul sur tous les processus que de le faire sur un seul et ensuite faire une diffusion\nEviter de lire des données en parallèle si vous n’utilisez pas une bibliothèque dédiée (MPI-IO). Lire le fichier sur le processeur 0 puis faire un bcast ou mieux un scatter.\nEssayer d’équilibrer la charge sur vos processus\nJeter un oeil à pdbMPI"
  },
  {
    "objectID": "exercice.html#recommendations-pour-utiliser-mpi",
    "href": "exercice.html#recommendations-pour-utiliser-mpi",
    "title": "Exercice",
    "section": "",
    "text": "Le modèle SPMD fonctionne mieux sur les clusters de calcul que le modèle SPMD\nLimiter au maximum les messages avec un grand volumes de données\nLimiter l’empreinte mémoire en divisant les calculs mais aussi en divisant la mémoire.\nIl est parfois plus intéressant de faire le même calcul sur tous les processus que de le faire sur un seul et ensuite faire une diffusion\nEviter de lire des données en parallèle si vous n’utilisez pas une bibliothèque dédiée (MPI-IO). Lire le fichier sur le processeur 0 puis faire un bcast ou mieux un scatter.\nEssayer d’équilibrer la charge sur vos processus\nJeter un oeil à pdbMPI"
  },
  {
    "objectID": "exercice.html#exercice",
    "href": "exercice.html#exercice",
    "title": "Exercice",
    "section": "Exercice",
    "text": "Exercice\nOn veut paralléliser l’exemple suivant avec Rmpi\nmy.loop &lt;- 20\nm.dim &lt;- list(nrow = 200000, ncol = 10)\nm &lt;- matrix(1, nrow = m.dim$nrow, ncol = m.dim$ncol)\nret &lt;- 0\n\nstart &lt;- Sys.time()\nfor(k in 1 : my.loop){\n  ret &lt;- ret + sum(rowSums(m))\n}\nSys.time() - start\nVoici la version MPMD, essayer d’en faire une version SPMD et comparer les performances.\n\nloop.fun &lt;- function(){\n  m.dim &lt;- list(nrow = 200000, ncol = 10)\n  m &lt;- matrix(1, nrow = m.dim$nrow, ncol = m.dim$ncol)\n  ret &lt;- sum(rowSums(m))\n}\n\ncall.mpi.master &lt;- function(){\n  library(Rmpi)\n  mpi.spawn.Rslaves(needlog = FALSE)\n  mpi.bcast.Robj2slave(call.mpi.slave)\n  mpi.bcast.cmd(call.mpi.slave())\n  mpi.bcast.Robj(loop.fun)\n\n  my.size &lt;- mpi.universe.size()\n  my.loop &lt;- 20 \n  my.split &lt;- data.frame(loop = 1 : my.loop,\n                         rank = sort(c(rep(1 : my.size, my.loop %/% my.size),\n                                if(my.loop %% my.size &gt; 0)\n                                (my.size : 2)[1 : (my.loop %% my.size)])))\n\n  for(i in 1 : my.size){\n    mpi.send.Robj(my.split$loop[my.split$rank == i], dest = i, tag = 1)\n  }\n\n  ret &lt;- 0\n  for(i in 1 : my.size){\n    ret &lt;- ret + mpi.recv(integer(1), type = 1, source = i, tag = 2)\n  }\n  ret\n}\n\ncall.mpi.slave &lt;- function(){\n  loop.fun &lt;- eval(mpi.bcast.Robj())\n  my.loop &lt;- mpi.recv.Robj(source = 0, tag = 1)\n\n  ret &lt;- 0\n  for(i in my.loop){\n    ret &lt;- ret + loop.fun()\n  }\n  mpi.send(as.integer(ret), type = 1, dest = 0, tag = 2)\n}\n\nstart &lt;- Sys.time()\ncall.mpi.master()\nSys.time() - start"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rmpi",
    "section": "",
    "text": "Site officiel de Rmpi\nSupports de cours MPI de l’IDRIS\nHow to run R programs on University of Maryland HPC facility\nDocumentation de GRICAD\nMPI Tutorial by Wes Kendall\nCRAN Task View: High-Performance and Parallel Computing with R\nR_note by Wei-Chen Chen"
  },
  {
    "objectID": "index.html#bibliographie",
    "href": "index.html#bibliographie",
    "title": "Rmpi",
    "section": "",
    "text": "Site officiel de Rmpi\nSupports de cours MPI de l’IDRIS\nHow to run R programs on University of Maryland HPC facility\nDocumentation de GRICAD\nMPI Tutorial by Wes Kendall\nCRAN Task View: High-Performance and Parallel Computing with R\nR_note by Wei-Chen Chen"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Rmpi",
    "section": "Introduction",
    "text": "Introduction\nL’utilisation de la bibliothèque MPI permet d’exploiter le parallélisme des ordinateurs en utilisant le paradigme de l’échange de messages.\nOn parle de programme séquentiel lorsqu’il est exécuté par un et un seul processus. Dans ce cas, toutes les variables et constantes sont allouées dans la mémoire allouée au processus\nDans un programme parallèle par échanges de messages\n\nLe programme est exécuté simultanément dans plusieurs processus.\nToutes les variables sont privées et résident dans la mémoire locale allouée à chaque processus.\nChaque processus exécute éventuellement des parties différentes d’un programme.\nUne donnée est échangée entre deux ou plusieurs processus via des appels de fonctions."
  },
  {
    "objectID": "index.html#léchange-de-messages",
    "href": "index.html#léchange-de-messages",
    "title": "Rmpi",
    "section": "L’échange de messages",
    "text": "L’échange de messages\nLe message est constitué de paquets de données transitant du processus émetteur au(x) processus récepteur(s). Il devra contenir:\n\nLes données (variables scalaires, tableaux, etc.)\nl’identificateur du processus émetteur\nle type de la donnée\nsa longueur\nl’identificateur du processus récepteur."
  },
  {
    "objectID": "index.html#architecture-des-supercalculateurs",
    "href": "index.html#architecture-des-supercalculateurs",
    "title": "Rmpi",
    "section": "Architecture des supercalculateurs",
    "text": "Architecture des supercalculateurs\nLa plupart des supercalculateurs sont des machines à mémoire distribuée. Ils sont composés d’un ensemble de nœud, à l’intérieur d’un nœud la mémoire est partagée."
  },
  {
    "objectID": "index.html#mpi-vs-openmp",
    "href": "index.html#mpi-vs-openmp",
    "title": "Rmpi",
    "section": "MPI vs OpenMP",
    "text": "MPI vs OpenMP\nOpenMP utilise un schéma à mémoire partagée, tandis que pour MPI la mémoire est distribuée."
  },
  {
    "objectID": "index.html#historique",
    "href": "index.html#historique",
    "title": "Rmpi",
    "section": "Historique",
    "text": "Historique\n\nVersion 1.0 : en juin 1994, le forum MPI, avec la participation d’une quarantaine d’organisations, aboutit à la définition d’un ensemble de sous-programmes concernant la bibliothèque d’échanges de messagesMPI\nVersion 2.0 : apparue en juillet 1997, cette version apportait des compléments importants volontairement non intégrés dans MPI 1.0 (gestion dynamique de processus, copies mémoire à mémoire, entrées-sorties parallèles, etc.)\nVersion 3.0 : septembre 2012, cette version apportait les communications collectives non bloquantes, nouvelle interface Fortran, etc.\nVersion 4.0 : juin 2021"
  },
  {
    "objectID": "index.html#implémentations-mpi",
    "href": "index.html#implémentations-mpi",
    "title": "Rmpi",
    "section": "Implémentations MPI",
    "text": "Implémentations MPI\n\nMPICH\nOpen MPI\n\nBibliothèques scientifiques parallèles\nHDF5 : Lecture et écriture sur fichiers."
  },
  {
    "objectID": "index.html#anatomie-dun-programme-mpi",
    "href": "index.html#anatomie-dun-programme-mpi",
    "title": "Rmpi",
    "section": "Anatomie d’un programme MPI",
    "text": "Anatomie d’un programme MPI\n\ninitialiser l’environnement : initialize\ncommunicateur : comm\nrang : rank\nnombre de processus : size\nfermer : finalize"
  },
  {
    "objectID": "index.html#exemple-c",
    "href": "index.html#exemple-c",
    "title": "Rmpi",
    "section": "Exemple C",
    "text": "Exemple C\n#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    // Initialize the MPI environment\n    MPI_Init(NULL, NULL);\n\n    // Get the number of processes\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    // Get the rank of the process\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Get the name of the processor\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &name_len);\n\n    // Print off a hello world message\n    printf(\"Hello world from processor %s, rank %d out of %d processors\\n\",\n           processor_name, world_rank, world_size);\n\n    // Finalize the MPI environment.\n    MPI_Finalize();\n}"
  },
  {
    "objectID": "index.html#compilation-et-exécution-dun-code-mpi-en-c",
    "href": "index.html#compilation-et-exécution-dun-code-mpi-en-c",
    "title": "Rmpi",
    "section": "Compilation et exécution d’un code MPI en C",
    "text": "Compilation et exécution d’un code MPI en C\nPour compiler un code MPI, il faut faire le lien avec la librairie MPI utilisée en utilisant par exemple mpicc\n&gt; mpicc hello_mpi.c -o hello\nPour exécuter un code MPI, on utilise un lanceur d’application MPI qui ordonne le lancement de l’exécution sur un nombre de processus choisi. Le lanceur défini par la norme MPI est mpiexec. Il existe également des lanceurs non standards, comme mpirun.\n&gt; mpiexec -n 4 ./hello\nHello world from processor ar039133.math.univ-rennes1.fr, rank 2 out of 4 processors\nHello world from processor ar039133.math.univ-rennes1.fr, rank 3 out of 4 processors\nHello world from processor ar039133.math.univ-rennes1.fr, rank 1 out of 4 processors\nHello world from processor ar039133.math.univ-rennes1.fr, rank 0 out of 4 processors"
  },
  {
    "objectID": "index.html#exemple-python",
    "href": "index.html#exemple-python",
    "title": "Rmpi",
    "section": "Exemple Python",
    "text": "Exemple Python\nPour les langages interprétés, il est nécessaire de lancer plusieurs sessions pour utiliser MPI.\n#!/usr/bin/env python\n\nfrom mpi4py import MPI\n\nsize = MPI.COMM_WORLD.Get_size()\nrank = MPI.COMM_WORLD.Get_rank()\nname = MPI.Get_processor_name()\n\nprintln(f\"Hello, World! I am process {rank} of {size} on {name}\")\n&gt; mpiexec -n 4 python hello_mpi.py                     \nHello, World! I am process 2 of 4 on ar039133.math.univ-rennes1.fr\nHello, World! I am process 1 of 4 on ar039133.math.univ-rennes1.fr\nHello, World! I am process 3 of 4 on ar039133.math.univ-rennes1.fr\nHello, World! I am process 0 of 4 on ar039133.math.univ-rennes1.fr"
  },
  {
    "objectID": "index.html#exemple-r-spmd",
    "href": "index.html#exemple-r-spmd",
    "title": "Rmpi",
    "section": "Exemple R SPMD",
    "text": "Exemple R SPMD\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm=0)\nnp &lt;- mpi.comm.size(comm=0)\nhostname &lt;- mpi.get.processor.name()\n\nmsg &lt;- sprintf (\"Hello world from task %03d of %03d, on host %s \\n\", id , np , hostname)\ncat(msg)\n\ninvisible(mpi.barrier(comm=0))\ninvisible(mpi.finalize())\nInstallation de l’environnement logiciel\n$ conda create -y -n rmpi r-rmpi -c conda-forge\n$ conda activate rmpi\n$ mpiexec -np 4 Rscript hello_mpi.R\nHello world from task 003 of 004, on host srv-mingus\nHello world from task 000 of 004, on host srv-mingus\nHello world from task 001 of 004, on host srv-mingus\nHello world from task 002 of 004, on host srv-mingus"
  },
  {
    "objectID": "index.html#exécution-sur-le-cluster-perseus",
    "href": "index.html#exécution-sur-le-cluster-perseus",
    "title": "Rmpi",
    "section": "Exécution sur le cluster perseus",
    "text": "Exécution sur le cluster perseus\nInstallation de Rmpi et récupération du matériel\nsource /applis/environments/conda.sh\nconda create -y -n rmpi r-rmpi -c conda-forge\nconda activate rmpi\ngit clone https://github.com/GroupeCalcul/ANFRCalculRmpi\ncd ANFRCalculRmpi\nfichier hello_mpi.sh nécessaire à l’utilisation d’OAR\n##!/bin/bash\n#OAR --project pr-groupecalcul\n#OAR -n hello_mpi\n#OAR -l /nodes=1/core=4,walltime=00:01:00\n#OAR --stdout hello_mpi.out\n#OAR --stderr hello_mpi.err\n\n## Ensure conda is loaded. The following line can be into your ~/.bashrc file.\nsource /applis/environments/conda.sh\n\n## Run the program\nconda activate rmpi\nmpirun --np 4 --machinefile $OAR_NODE_FILE --mca plm_rsh_agent \"oarsh\" $OAR_WORKING_DIRECTORY/hello_mpi\nmpirun --np 4 --machinefile $OAR_NODE_FILE --mca plm_rsh_agent \"oarsh\" Rscript $OAR_WORKING_DIRECTORY/hello_mpi.R\nCe script doit être exécutable\n$ chmod +x hello_mpi.sh\n$ ls -l hello_mpi.sh\n-rwxr-xr-x 1 login-perseus l-formations 567 Sep 24 14:30 hello_mpi.sh\n$ oarsub -S ./hello_mpi.sh\n[ADMISSION RULE] Antifragmentation activated\n[ADMISSION RULE] No antifrag for small jobs\n[FAST] Adding fast resource constraints\n[PARALLEL] Small jobs (&lt; 32 cores) restricted to tagged nodes\n[ADMISSION RULE] Modify resource description with type constraints\n[ADMISSION RULE] Found job type [verysmall]\n[ADMISSION RULE] Automatically add job type [verysmall]\nOAR_JOB_ID=26118495\n$ oarstat -u\nJob id    S User     Duration   System message\n--------- - -------- ---------- ------------------------------------------------\n26118496  W navarop-    0:00:00 R=4,W=0:1:0,J=B,N=hello_rmpi,P=groupecalcul,T=heterogeneous|verysmall\n$ oarstat -u\nJob id    S User     Duration   System message\n--------- - -------- ---------- ------------------------------------------------\n26118496  L navarop-    0:00:03 R=4,W=0:1:0,J=B,N=hello_rmpi,P=groupecalcul,T=heterogeneous|verysmall (Karma=0.004,quota_ok)\n$ oarstat -u\nJob id    S User     Duration   System message\n--------- - -------- ---------- ------------------------------------------------\n26118496  R navarop-    0:00:06 R=4,W=0:1:0,J=B,N=hello_rmpi,P=groupecalcul,T=heterogeneous|verysmall (Karma=0.004,quota_ok)\n$ oarstat -u\nJob id    S User     Duration   System message\n--------- - -------- ---------- ------------------------------------------------\n26118496  F navarop-    0:00:18 R=4,W=0:1:0,J=B,N=hello_rmpi,P=groupecalcul,T=heterogeneous|verysmall (Karma=0.004,quota_ok)\n$ cat hello_mpi.out\nHello world from processor dahu146, rank 3 out of 4 processors\nHello world from processor dahu146, rank 2 out of 4 processors\nHello world from processor dahu146, rank 0 out of 4 processors\nHello world from processor dahu146, rank 1 out of 4 processors\nHello world from task 003 of 004, on host dahu146\nHello world from task 000 of 004, on host dahu146\nHello world from task 002 of 004, on host dahu146\nHello world from task 001 of 004, on host dahu146"
  },
  {
    "objectID": "index.html#exemple-r-mpmd",
    "href": "index.html#exemple-r-mpmd",
    "title": "Rmpi",
    "section": "Exemple R MPMD",
    "text": "Exemple R MPMD\n\nlibrary(Rmpi)\n\nmpi.spawn.Rslaves(nslaves = 3, needlog = FALSE)\n\nmpi.bcast.cmd( id &lt;- mpi.comm.rank() )\nmpi.bcast.cmd( np &lt;- mpi.comm.size() )\nmpi.bcast.cmd( host &lt;- mpi.get.processor.name() )\nresult &lt;- mpi.remote.exec(paste(\"I am\", id, \"of\", np, \"running on\", host)) \n\nprint(unlist(result))\n\nmpi.close.Rslaves(dellog = FALSE)\nmpi.exit()\n$ Rscript hello_mpmd.R"
  },
  {
    "objectID": "index.html#spmd-vs-mpmd",
    "href": "index.html#spmd-vs-mpmd",
    "title": "Rmpi",
    "section": "SPMD vs MPMD",
    "text": "SPMD vs MPMD\n\nPlus proche du code séquentiel, c’est-à-dire que le SPMD est plus facile à coder à partir de la version séquentielle\nPlus court que la version MPMD, donc moins d’erreurs potentielles.\nLe processeur 0 travaille également, ce qui permet d’utiliser pleinement les ressources.\nEn général la taille et nombre de messages sont plus réduites."
  },
  {
    "objectID": "index.html#mpi-exercice-1-environnement-mpi",
    "href": "index.html#mpi-exercice-1-environnement-mpi",
    "title": "Rmpi",
    "section": "MPI – Exercice 1 : Environnement MPI",
    "text": "MPI – Exercice 1 : Environnement MPI\nImplémenter un programme MPI SPMD dans lequel chaque processus affiche un message indiquant si son rang est pair ou impair. Par exemple :\n&gt; mpiexec -np 4 Rscript pair_impair.R\nMoi, processus 0, je suis de rang pair\nMoi, processus 1, je suis de rang impair\nMoi, processus 2, je suis de rang pair\nMoi, processus 3, je suis de rang impair"
  },
  {
    "objectID": "com_point_a_point.html",
    "href": "com_point_a_point.html",
    "title": "Communications point à point",
    "section": "",
    "text": "Une communication dite point à point a lieu entre deux processus, l’un appelé processus émetteur et l’autre processus récepteur (ou destinataire). L’émetteur et le récepteur sont identifiés par leur rang dans le communicateur. L’entité transmise entre deux processus est appelée message. Un message est caractérisé par son enveloppe. Celle-ci est constituée :\n\ndu rang du processus émetteur ;\ndu rang du processus récepteur ;\nde l’étiquette (tag) du message ;\ndu communicateur qui définit le groupe de processus et le contexte de communication.\nLes données échangées sont typées (entiers, réels, etc…)\n\nCette opération est bloquante : l’exécution reste bloquée jusqu’à ce que le contenu de message puisse être réécrit sans risque d’écraser la valeur qui devait être envoyée.\n\nmpi.send(x, type, dest, tag,  comm = 1)\nmpi.recv(x, type, source, tag,  comm = 1, status = 0)\n\ntype : 1 pour les entiers, 2 pour les flottants, 3 pour les caractères.\nL’appel mpi.recv pourra fonctionner avec une opération mpi.send si ces deux appels ont la même enveloppe (rang_source, rang_dest, etiquette, comm).\nCette opération est bloquante : l’exécution reste bloquée jusqu’à ce que le contenu de message corresponde au message reçu.\n\n\nlibrary(Rmpi)\n\nid &lt;- mpi.comm.rank(comm = 0)\nnp &lt;- mpi.comm.size(comm = 0)\n\nif (id == 0) {\n    for (i in seq(1, np-1)) {\n        recv &lt;- mpi.recv(x = integer(1), type = 1, source = i, tag = 0, comm = 0)\n        cat(\"Process 0: Received from process\", i, \"saying:\", recv, \"\\n\")\n    }\n} else {\n    msg &lt;- as.integer(id)\n    cat(\"Process\", id, \" sent to process 0:\", msg, \"\\n\")\n    invisible(mpi.send(msg, 1, dest = 0, tag = 0, comm = 0))\n}\n\ninvisible(mpi.barrier(comm = 0))\ninvisible(mpi.finalize())\n$ mpiexec -np 4 Rscript send_recv.R\nProcess 3  sent to process 0: 3\nProcess 2  sent to process 0: 2\nProcess 1  sent to process 0: 1\nProcess 0: Received from process 1 saying: 1\nProcess 0: Received from process 2 saying: 2\nProcess 0: Received from process 3 saying: 3"
  },
  {
    "objectID": "com_point_a_point.html#exemple",
    "href": "com_point_a_point.html#exemple",
    "title": "Communications point à point",
    "section": "",
    "text": "library(Rmpi)\n\nid &lt;- mpi.comm.rank(comm = 0)\nnp &lt;- mpi.comm.size(comm = 0)\n\nif (id == 0) {\n    for (i in seq(1, np-1)) {\n        recv &lt;- mpi.recv(x = integer(1), type = 1, source = i, tag = 0, comm = 0)\n        cat(\"Process 0: Received from process\", i, \"saying:\", recv, \"\\n\")\n    }\n} else {\n    msg &lt;- as.integer(id)\n    cat(\"Process\", id, \" sent to process 0:\", msg, \"\\n\")\n    invisible(mpi.send(msg, 1, dest = 0, tag = 0, comm = 0))\n}\n\ninvisible(mpi.barrier(comm = 0))\ninvisible(mpi.finalize())\n$ mpiexec -np 4 Rscript send_recv.R\nProcess 3  sent to process 0: 3\nProcess 2  sent to process 0: 2\nProcess 1  sent to process 0: 1\nProcess 0: Received from process 1 saying: 1\nProcess 0: Received from process 2 saying: 2\nProcess 0: Received from process 3 saying: 3"
  }
]